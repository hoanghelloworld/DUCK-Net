{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import albumentations as albu\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from ModelArchitecture.DiceLoss import dice_metric_loss\n",
    "from ModelArchitecture import DUCK_Net\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852dbbf-f3a4-4db6-8c5e-98889bf3abd4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Checking the number of GPUs available\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabacd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient data loading functions using TensorFlow data API\n",
    "\n",
    "def get_file_paths(train_path, mask_path, test_path=None):\n",
    "    \"\"\"\n",
    "    Get file paths instead of loading all images into memory\n",
    "    \n",
    "    Parameters:\n",
    "    train_path: Path to training images\n",
    "    mask_path: Path to mask images\n",
    "    test_path: Path to test images (optional)\n",
    "    \n",
    "    Returns:\n",
    "    train_files: List of training image paths\n",
    "    mask_files: List of mask image paths\n",
    "    test_files: List of test image paths (if test_path provided)\n",
    "    \"\"\"\n",
    "    # Get all image files from directories\n",
    "    train_files = sorted([os.path.join(train_path, f) for f in os.listdir(train_path) if f.endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif'))])\n",
    "    mask_files = sorted([os.path.join(mask_path, f) for f in os.listdir(mask_path) if f.endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif'))])\n",
    "    \n",
    "    print(f\"Found {len(train_files)} training images and {len(mask_files)} mask images\")\n",
    "    \n",
    "    test_files = None\n",
    "    if test_path:\n",
    "        test_files = sorted([os.path.join(test_path, f) for f in os.listdir(test_path) if f.endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif'))])\n",
    "        print(f\"Found {len(test_files)} test images\")\n",
    "    \n",
    "    return train_files, mask_files, test_files\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load and preprocess a single image\"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "def load_mask(mask_path):\n",
    "    \"\"\"Load and preprocess a single mask\"\"\"\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_image(mask, channels=1)\n",
    "    # Convert to binary\n",
    "    mask = tf.cast(mask > 127, tf.float32)\n",
    "    return mask\n",
    "\n",
    "def create_dataset(image_paths, mask_paths=None, batch_size=4, shuffle=True, augment=False):\n",
    "    \"\"\"Create a TensorFlow dataset that loads images on-the-fly\"\"\"\n",
    "    if mask_paths is not None:\n",
    "        # Training/validation dataset with images and masks\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
    "        dataset = dataset.map(lambda x, y: (load_image(x), load_mask(y)), \n",
    "                             num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        # Test dataset with only images\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "        dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    # Use prefetch to overlap data preprocessing and model execution\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def augment_batch(images, masks):\n",
    "    \"\"\"Apply augmentations to a batch of images and masks\"\"\"\n",
    "    # Convert to numpy for albumentations\n",
    "    images_np = images.numpy()\n",
    "    masks_np = masks.numpy()\n",
    "    \n",
    "    aug_images = []\n",
    "    aug_masks = []\n",
    "    \n",
    "    # Process each image-mask pair in the batch\n",
    "    for img, mask in zip(images_np, masks_np):\n",
    "        aug = aug_train(image=img, mask=mask)\n",
    "        aug_images.append(aug['image'])\n",
    "        aug_masks.append(aug['mask'])\n",
    "    \n",
    "    return np.array(aug_images), np.array(aug_masks)\n",
    "\n",
    "# Legacy function for compatibility with existing code\n",
    "def load_custom_data(train_path, mask_path, test_path=None):\n",
    "    \"\"\"\n",
    "    Return file paths instead of loaded images\n",
    "    \"\"\"\n",
    "    return get_file_paths(train_path, mask_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model parameters\n",
    "\n",
    "# Image dimensions for the custom dataset (600x450)\n",
    "img_height = 600\n",
    "img_width = 450\n",
    "\n",
    "learning_rate = 1e-4\n",
    "seed_value = 58800\n",
    "filters = 17  # Number of filters, the paper presents the results with 17 and 34\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "ct = datetime.now()\n",
    "\n",
    "model_type = \"DuckNet\"\n",
    "\n",
    "# Define paths for your dataset\n",
    "data_root = '/workspaces/DUCK-Net/data'  # Update this path to your data directory\n",
    "train_path = os.path.join(data_root, 'train')\n",
    "mask_path = os.path.join(data_root, 'mask')\n",
    "test_path = os.path.join(data_root, 'test')\n",
    "output_path = os.path.join(data_root, 'predictions')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Paths for saving model and progress\n",
    "progress_path = 'ProgressFull/custom_progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct) + '.csv'\n",
    "progressfull_path = 'ProgressFull/custom_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt'\n",
    "plot_path = 'ProgressFull/custom_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png'\n",
    "model_path = 'ModelSaveTensorFlow/custom/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
    "\n",
    "EPOCHS = 600\n",
    "min_loss_for_saving = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147d0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths instead of loading all images at once\n",
    "train_files, mask_files, test_files = load_custom_data(train_path, mask_path, test_path)\n",
    "print(f\"Training files: {len(train_files)}, Mask files: {len(mask_files)}\")\n",
    "if test_files is not None:\n",
    "    print(f\"Test files: {len(test_files)}\")\n",
    "\n",
    "# Count number of samples for metrics calculation\n",
    "num_train_samples = len(train_files)\n",
    "num_test_samples = len(test_files) if test_files is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data paths, seed for reproducibility\n",
    "train_files_train, train_files_valid, mask_files_train, mask_files_valid = train_test_split(\n",
    "    train_files, mask_files, test_size=0.2, shuffle=True, random_state=seed_value\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_files_train)} images, Validation set: {len(train_files_valid)} images\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_files_train, mask_files_train, batch_size=4, shuffle=True)\n",
    "valid_dataset = create_dataset(train_files_valid, mask_files_valid, batch_size=4, shuffle=False)\n",
    "\n",
    "if test_files is not None:\n",
    "    test_dataset = create_dataset(test_files, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the augmentations (keeping the existing code)\n",
    "aug_train = albu.Compose([\n",
    "    albu.HorizontalFlip(),\n",
    "    albu.VerticalFlip(),\n",
    "    albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
    "    albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
    "])\n",
    "\n",
    "# Create a TF function for augmentation\n",
    "@tf.function\n",
    "def augment_dataset(images, masks):\n",
    "    # Use tf.py_function to call numpy-based augmentation\n",
    "    [aug_images, aug_masks] = tf.py_function(\n",
    "        augment_batch, [images, masks], [tf.float32, tf.float32]\n",
    "    )\n",
    "    # Ensure shapes are preserved\n",
    "    aug_images.set_shape(images.shape)\n",
    "    aug_masks.set_shape(masks.shape)\n",
    "    return aug_images, aug_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609dd32",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "model = DUCK_Net.create_model(img_height=img_height, img_width=img_width, input_chanels=3, out_classes=1, starting_filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e513d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=dice_metric_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with memory-efficient data loading\n",
    "# No augmentation - using default dataset\n",
    "\n",
    "step = 0\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    print(f'Training, epoch {epoch}')\n",
    "    print('Learning Rate: ' + str(learning_rate))\n",
    "\n",
    "    step += 1\n",
    "    \n",
    "    # For each epoch, we'll iterate through the dataset\n",
    "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
    "    \n",
    "    # Train for one epoch using the original dataset without augmentation\n",
    "    history = model.fit(\n",
    "        train_dataset,  # Using the original dataset without augmentation\n",
    "        epochs=1,\n",
    "        validation_data=valid_dataset,\n",
    "        verbose=1,\n",
    "        callbacks=[csv_logger]\n",
    "    )\n",
    "    \n",
    "    # Validate on the entire validation set\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for x_batch, y_batch in valid_dataset:\n",
    "        pred_batch = model.predict(x_batch, verbose=0)\n",
    "        batch_loss = dice_metric_loss(y_batch, pred_batch).numpy()\n",
    "        val_loss += batch_loss\n",
    "        val_batches += 1\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    print(\"Loss Validation: \" + str(avg_val_loss))\n",
    "    \n",
    "    with open(progressfull_path, 'a') as f:\n",
    "        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(avg_val_loss) + '\\n\\n\\n')\n",
    "    \n",
    "    # Save model if improved\n",
    "    if min_loss_for_saving > avg_val_loss:\n",
    "        min_loss_for_saving = avg_val_loss\n",
    "        print(\"Saved model with val_loss: \", avg_val_loss)\n",
    "        model.save(model_path)\n",
    "    \n",
    "    # Clear memory between epochs\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing metrics on training and validation sets with batch processing\n",
    "\n",
    "print(\"Loading the best model\")\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss': dice_metric_loss})\n",
    "\n",
    "# Collect predictions and ground truth in batches\n",
    "y_true_train = []\n",
    "y_pred_train = []\n",
    "y_true_valid = []\n",
    "y_pred_valid = []\n",
    "\n",
    "print(\"Computing metrics for training set...\")\n",
    "for x_batch, y_batch in create_dataset(train_files_train, mask_files_train, batch_size=4, shuffle=False):\n",
    "    pred_batch = model.predict(x_batch, verbose=0)\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    pred_batch_binary = (pred_batch > 0.5).astype(np.float32)\n",
    "    \n",
    "    # Flatten and append\n",
    "    y_true_train.extend(y_batch.numpy().flatten())\n",
    "    y_pred_train.extend(pred_batch_binary.flatten())\n",
    "\n",
    "print(\"Computing metrics for validation set...\")\n",
    "for x_batch, y_batch in create_dataset(train_files_valid, mask_files_valid, batch_size=4, shuffle=False):\n",
    "    pred_batch = model.predict(x_batch, verbose=0)\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    pred_batch_binary = (pred_batch > 0.5).astype(np.float32)\n",
    "    \n",
    "    # Flatten and append\n",
    "    y_true_valid.extend(y_batch.numpy().flatten())\n",
    "    y_pred_valid.extend(pred_batch_binary.flatten())\n",
    "\n",
    "# Calculate metrics\n",
    "y_true_train = np.array(y_true_train, dtype=bool)\n",
    "y_pred_train = np.array(y_pred_train, dtype=bool)\n",
    "y_true_valid = np.array(y_true_valid, dtype=bool)\n",
    "y_pred_valid = np.array(y_pred_valid, dtype=bool)\n",
    "\n",
    "dice_train = f1_score(y_true_train, y_pred_train)\n",
    "dice_valid = f1_score(y_true_valid, y_pred_valid)\n",
    "\n",
    "miou_train = jaccard_score(y_true_train, y_pred_train)\n",
    "miou_valid = jaccard_score(y_true_valid, y_pred_valid)\n",
    "\n",
    "precision_train = precision_score(y_true_train, y_pred_train)\n",
    "precision_valid = precision_score(y_true_valid, y_pred_valid)\n",
    "\n",
    "recall_train = recall_score(y_true_train, y_pred_train)\n",
    "recall_valid = recall_score(y_true_valid, y_pred_valid)\n",
    "\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "accuracy_valid = accuracy_score(y_true_valid, y_pred_valid)\n",
    "\n",
    "# Print and save metrics\n",
    "print(f\"Dice score - Train: {dice_train:.4f}, Validation: {dice_valid:.4f}\")\n",
    "print(f\"IoU score  - Train: {miou_train:.4f}, Validation: {miou_valid:.4f}\")\n",
    "print(f\"Precision  - Train: {precision_train:.4f}, Validation: {precision_valid:.4f}\")\n",
    "print(f\"Recall     - Train: {recall_train:.4f}, Validation: {recall_valid:.4f}\")\n",
    "print(f\"Accuracy   - Train: {accuracy_train:.4f}, Validation: {accuracy_valid:.4f}\")\n",
    "\n",
    "final_file = 'results_' + model_type + '_' + str(filters) + '_custom.txt'\n",
    "with open(final_file, 'a') as f:\n",
    "    f.write('Custom Dataset\\n\\n')\n",
    "    f.write(f'dice_train: {dice_train:.4f} dice_valid: {dice_valid:.4f}\\n\\n')\n",
    "    f.write(f'miou_train: {miou_train:.4f} miou_valid: {miou_valid:.4f}\\n\\n')\n",
    "    f.write(f'precision_train: {precision_train:.4f} precision_valid: {precision_valid:.4f}\\n\\n')\n",
    "    f.write(f'recall_train: {recall_train:.4f} recall_valid: {recall_valid:.4f}\\n\\n')\n",
    "    f.write(f'accuracy_train: {accuracy_train:.4f} accuracy_valid: {accuracy_valid:.4f}\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test images and saving results with batch processing\n",
    "\n",
    "if test_files is not None:\n",
    "    print(f\"Predicting on {len(test_files)} test images...\")\n",
    "    \n",
    "    # Get the list of test image filenames\n",
    "    test_images = [os.path.basename(f) for f in test_files]\n",
    "    \n",
    "    # Create dataset for test images\n",
    "    test_dataset = create_dataset(test_files, batch_size=4, shuffle=False)\n",
    "    \n",
    "    # Process batches\n",
    "    batch_idx = 0\n",
    "    for batch in test_dataset:\n",
    "        # Get predictions for this batch\n",
    "        batch_preds = model.predict(batch, verbose=0)\n",
    "        \n",
    "        # Process each prediction in the batch\n",
    "        for i in range(batch_preds.shape[0]):\n",
    "            # Get the image index in the overall dataset\n",
    "            img_idx = batch_idx * 4 + i\n",
    "            \n",
    "            # Skip if we've exceeded the number of test images\n",
    "            if img_idx >= len(test_images):\n",
    "                break\n",
    "                \n",
    "            # Convert prediction to binary mask\n",
    "            pred_mask = (batch_preds[i] > 0.5).astype(np.uint8) * 255\n",
    "            \n",
    "            # Save the mask\n",
    "            output_filename = os.path.join(output_path, test_images[img_idx])\n",
    "            cv2.imwrite(output_filename, pred_mask)\n",
    "        \n",
    "        # Increment batch index\n",
    "        batch_idx += 1\n",
    "        \n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "else:\n",
    "    print(\"No test data provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb81e8",
   "metadata": {},
   "source": [
    "# Visualization of Results\n",
    "\n",
    "Let's visualize some of the test predictions alongside the original images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936358af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few test images and their predictions with batch processing\n",
    "if test_files is not None:\n",
    "    num_samples = min(5, len(test_files))\n",
    "    plt.figure(figsize=(12, 4*num_samples))\n",
    "    \n",
    "    # Create a dataset with just the first few test images\n",
    "    sample_test_files = test_files[:num_samples]\n",
    "    sample_test_dataset = create_dataset(sample_test_files, batch_size=num_samples, shuffle=False)\n",
    "    \n",
    "    # Get a single batch containing all samples\n",
    "    for test_batch in sample_test_dataset:\n",
    "        # This will execute only once since we created a dataset with exactly one batch\n",
    "        sample_images = test_batch.numpy()\n",
    "        sample_preds = model.predict(test_batch, verbose=0)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Original image\n",
    "            plt.subplot(num_samples, 2, i*2+1)\n",
    "            plt.imshow(sample_images[i])\n",
    "            plt.title(f\"Test Image {i+1}\")\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "            # Prediction\n",
    "            plt.subplot(num_samples, 2, i*2+2)\n",
    "            plt.imshow(sample_preds[i, :, :, 0] > 0.5, cmap='gray')\n",
    "            plt.title(f\"Prediction {i+1}\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "        break  # Only process the first batch\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, \"sample_predictions.png\"))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
