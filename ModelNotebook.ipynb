{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "# import albumentations as albu # Not needed as augmentations are removed\n",
    "import numpy as np\n",
    "import gc\n",
    "# import pickle # Not used in the provided snippet\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from ModelArchitecture.DiceLoss import dice_metric_loss\n",
    "from ModelArchitecture import DUCK_Net\n",
    "from ImageLoader import ImageLoader2D\n",
    "from functools import partial # Added for tf.data.Dataset.from_generator\n",
    "import os # Added for path joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852dbbf-f3a4-4db6-8c5e-98889bf3abd4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Checking the number of GPUs available\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model parameters\n",
    "\n",
    "IMG_HEIGHT = 608\n",
    "IMG_WIDTH = 448\n",
    "BATCH_SIZE = 4 # Adjust based on your GPU memory\n",
    "\n",
    "# IMPORTANT: Set this path to the root directory containing your 'train', 'masks', and 'test' subfolders\n",
    "DATA_FOLDER_PATH = \"/workspaces/DUCK-Net/sample_data/\" # Replace with your actual data path\n",
    "\n",
    "dataset_name_suffix = f\"custom_h{IMG_HEIGHT}_w{IMG_WIDTH}\"\n",
    "learning_rate = 1e-4\n",
    "seed_value = 58800 # For reproducibility of splits\n",
    "filters = 17 # Number of filters, the paper presents the results with 17 and 34\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "ct = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "model_type = \"DuckNet\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('ProgressFull', exist_ok=True)\n",
    "os.makedirs(f'ModelSaveTensorFlow/{dataset_name_suffix}', exist_ok=True)\n",
    "\n",
    "progress_path = f'ProgressFull/{dataset_name_suffix}_progress_csv_{model_type}_filters_{str(filters)}_{ct}.csv'\n",
    "progressfull_path = f'ProgressFull/{dataset_name_suffix}_progress_{model_type}_filters_{str(filters)}_{ct}.txt'\n",
    "# plot_path = f'ProgressFull/{dataset_name_suffix}_progress_plot_{model_type}_filters_{str(filters)}_{ct}.png' # Plotting code not shown, can be added if needed\n",
    "model_path = f'ModelSaveTensorFlow/{dataset_name_suffix}/{model_type}_filters_{str(filters)}_{ct}'\n",
    "\n",
    "EPOCHS = 100 # Adjust as needed, original was 600\n",
    "min_loss_for_saving = 0.2 # Initial high value, will be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264050d8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data paths\n",
    "\n",
    "print(f\"Loading data from: {DATA_FOLDER_PATH}\")\n",
    "# ImageLoader2D.folder_path = DATA_FOLDER_PATH # Set if ImageLoader2D uses a global var, otherwise pass as arg\n",
    "all_train_img_files, all_train_mask_files, final_test_img_files = ImageLoader2D.get_image_paths(DATA_FOLDER_PATH)\n",
    "\n",
    "print(f\"Found {len(all_train_img_files)} training images and {len(all_train_mask_files)} training masks.\")\n",
    "print(f\"Found {len(final_test_img_files)} final test images.\")\n",
    "\n",
    "if not all_train_img_files or not all_train_mask_files:\n",
    "    raise ValueError(\"No training images or masks found. Check DATA_FOLDER_PATH and data structure.\")\n",
    "\n",
    "# Ensure the number of images and masks match before splitting\n",
    "if len(all_train_img_files) != len(all_train_mask_files):\n",
    "    raise ValueError(f\"Mismatch between number of training images ({len(all_train_img_files)}) and masks ({len(all_train_mask_files)}). Please check your dataset and ImageLoader2D.get_image_paths logic.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data paths for training, validation, and a hold-out test set (from training data)\n",
    "\n",
    "# Split full training data into training + validation set and a hold-out test set\n",
    "# Using 80% for train+val, 20% for hold-out test from the original training data\n",
    "train_val_img_paths, hold_out_test_img_paths, train_val_mask_paths, hold_out_test_mask_paths = train_test_split(\n",
    "    all_train_img_files, all_train_mask_files, test_size=0.2, random_state=seed_value, shuffle=True\n",
    ")\n",
    "\n",
    "# Split training + validation set into actual training and validation sets\n",
    "# Using ~80% of train_val for training, ~20% for validation (which is 16% of total original training data)\n",
    "train_img_paths, val_img_paths, train_mask_paths, val_mask_paths = train_test_split(\n",
    "    train_val_img_paths, train_val_mask_paths, test_size=0.2, random_state=seed_value, shuffle=True # 0.2 of 0.8 is 0.16\n",
    ")\n",
    "\n",
    "print(f\"Training images: {len(train_img_paths)}\")\n",
    "print(f\"Validation images: {len(val_img_paths)}\")\n",
    "print(f\"Hold-out test images (from training data): {len(hold_out_test_img_paths)}\")\n",
    "print(f\"Final test images (unlabeled): {len(final_test_img_files)}\")\n",
    "\n",
    "\n",
    "# Define output signatures for tf.data.Dataset\n",
    "output_signature_train = (\n",
    "    tf.TensorSpec(shape=(IMG_HEIGHT, IMG_WIDTH, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(IMG_HEIGHT, IMG_WIDTH, 1), dtype=tf.uint8)\n",
    ")\n",
    "output_signature_test = tf.TensorSpec(shape=(IMG_HEIGHT, IMG_WIDTH, 3), dtype=tf.float32)\n",
    "\n",
    "# Create tf.data.Dataset objects\n",
    "# Partial function to pass fixed arguments to the generator\n",
    "train_generator = partial(ImageLoader2D.tf_dataset_generator, \n",
    "                          img_height=IMG_HEIGHT, img_width=IMG_WIDTH, is_test_set=False)\n",
    "test_generator = partial(ImageLoader2D.tf_dataset_generator, \n",
    "                         img_height=IMG_HEIGHT, img_width=IMG_WIDTH, is_test_set=True)\n",
    "\n",
    "if train_img_paths:\n",
    "    train_ds = tf.data.Dataset.from_generator(\n",
    "        lambda: train_generator(image_paths=train_img_paths, mask_paths=train_mask_paths),\n",
    "        output_signature=output_signature_train\n",
    "    ).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "else:\n",
    "    train_ds = None\n",
    "    print(\"Warning: No training data after split.\")\n",
    "\n",
    "if val_img_paths:\n",
    "    val_ds = tf.data.Dataset.from_generator(\n",
    "        lambda: train_generator(image_paths=val_img_paths, mask_paths=val_mask_paths),\n",
    "        output_signature=output_signature_train\n",
    "    ).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "else:\n",
    "    val_ds = None\n",
    "    print(\"Warning: No validation data after split.\")\n",
    "\n",
    "if hold_out_test_img_paths:\n",
    "    hold_out_test_ds = tf.data.Dataset.from_generator(\n",
    "        lambda: train_generator(image_paths=hold_out_test_img_paths, mask_paths=hold_out_test_mask_paths),\n",
    "        output_signature=output_signature_train\n",
    "    ).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "else:\n",
    "    hold_out_test_ds = None\n",
    "    print(\"Warning: No hold-out test data after split.\")\n",
    "\n",
    "if final_test_img_files:\n",
    "    final_test_ds = tf.data.Dataset.from_generator(\n",
    "        lambda: test_generator(image_paths=final_test_img_files, mask_paths=None), # mask_paths is None for test_generator\n",
    "        output_signature=output_signature_test\n",
    "    ).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "else:\n",
    "    final_test_ds = None\n",
    "    print(\"Warning: No final (unlabeled) test data found.\")\n",
    "\n",
    "if train_ds is None:\n",
    "    raise ValueError(\"Training dataset is empty. Cannot proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations are removed as per user request.\n",
    "# This cell can be deleted or left empty.\n",
    "# Original content:\n",
    "# aug_train = albu.Compose([\n",
    "#     albu.HorizontalFlip(),\n",
    "#     albu.VerticalFlip(),\n",
    "#     albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
    "#     albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
    "# ])\n",
    "\n",
    "# def augment_images():\n",
    "#     x_train_out = []\n",
    "#     y_train_out = []\n",
    "\n",
    "#     for i in range (len(x_train)):\n",
    "#         ug = aug_train(image=x_train[i], mask=y_train[i])\n",
    "#         x_train_out.append(ug['image'])  \n",
    "#         y_train_out.append(ug['mask'])\n",
    "\n",
    "#     return np.array(x_train_out), np.array(y_train_out)\n",
    "print(\"Augmentation cell skipped as per requirement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609dd32",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "model = DUCK_Net.create_model(img_height=IMG_HEIGHT, img_width=IMG_WIDTH, input_chanels=3, out_classes=1, starting_filters=filters)\n",
    "model.summary() # Display model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e513d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=dice_metric_loss, metrics=[dice_metric_loss]) # Added dice_metric_loss also as a metric for easier logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef712b9",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "# step = 0 # step variable not used in the new loop structure\n",
    "\n",
    "csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
    "history_list = [] # To store history from each epoch training\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nTraining, epoch {epoch+1}/{EPOCHS}')\n",
    "    print('Learning Rate: ' + str(tf.keras.backend.get_value(model.optimizer.learning_rate))) # Get current LR\n",
    "\n",
    "    # No augmentation needed, fit directly on train_ds\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=1, # Training one epoch at a time in this loop to allow custom actions between epochs\n",
    "        validation_data=val_ds, \n",
    "        callbacks=[csv_logger], # CSVLogger will append, ensure it's initialized outside the loop\n",
    "        verbose=1\n",
    "    )\n",
    "    history_list.append(history.history)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    if val_ds:\n",
    "        val_results = model.evaluate(val_ds, verbose=0)\n",
    "        loss_valid = val_results[0] # Assuming loss is the first item\n",
    "        print(f\"Validation Loss: {loss_valid:.4f}\")\n",
    "        if len(val_results) > 1:\n",
    "             print(f\"Validation Dice Metric: {val_results[1]:.4f}\") # Assuming dice_metric_loss is the second item (first metric)\n",
    "    else:\n",
    "        loss_valid = float('inf') # No validation set, cannot determine validation loss\n",
    "        print(\"No validation set to evaluate.\")\n",
    "        \n",
    "    # Evaluate on hold-out test set (from training data)\n",
    "    if hold_out_test_ds:\n",
    "        test_results = model.evaluate(hold_out_test_ds, verbose=0)\n",
    "        loss_test = test_results[0]\n",
    "        print(f\"Hold-out Test Loss: {loss_test:.4f}\")\n",
    "        if len(test_results) > 1:\n",
    "            print(f\"Hold-out Test Dice Metric: {test_results[1]:.4f}\")\n",
    "\n",
    "    else:\n",
    "        loss_test = float('inf')\n",
    "        print(\"No hold-out test set to evaluate.\")\n",
    "        \n",
    "    with open(progressfull_path, 'a') as f:\n",
    "        f.write(f'epoch: {epoch}\\nval_loss: {loss_valid:.4f}\\ntest_loss: {loss_test:.4f}\\n\\n')\n",
    "    \n",
    "    if val_ds and loss_valid < min_loss_for_saving: # Save based on validation loss\n",
    "        min_loss_for_saving = loss_valid\n",
    "        print(f\"Improved validation loss to {min_loss_for_saving:.4f}. Saving model to {model_path}\")\n",
    "        model.save(model_path)\n",
    "    elif not val_ds and epoch % 10 == 0: # If no validation set, save periodically\n",
    "        print(f\"No validation set. Saving model at epoch {epoch} to {model_path}\")\n",
    "        model.save(model_path)\n",
    "        \n",
    "    gc.collect() # Garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd52447",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Computing the metrics and saving the results\n",
    "\n",
    "print(f\"Loading the best model from: {model_path}\")\n",
    "# Ensure custom objects are passed if model was saved with custom loss/metrics\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss': dice_metric_loss})\n",
    "\n",
    "# Helper function to get all predictions and labels from a tf.data.Dataset\n",
    "def get_all_preds_labels(dataset, model_to_eval):\n",
    "    all_labels_list = []\n",
    "    all_preds_list = []\n",
    "    if dataset is None:\n",
    "        return np.array([]), np.array([])\n",
    "        \n",
    "    for images, labels in dataset: # Assumes dataset yields (images, labels)\n",
    "        preds = model_to_eval.predict_on_batch(images)\n",
    "        all_labels_list.append(labels.numpy())\n",
    "        all_preds_list.append(preds)\n",
    "    \n",
    "    if not all_labels_list: # Handle empty dataset case\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    return np.concatenate(all_labels_list, axis=0), np.concatenate(all_preds_list, axis=0)\n",
    "\n",
    "print(\"Evaluating on training data...\")\n",
    "y_train_np, pred_train_np = get_all_preds_labels(train_ds, model)\n",
    "print(\"Evaluating on validation data...\")\n",
    "y_valid_np, pred_valid_np = get_all_preds_labels(val_ds, model)\n",
    "print(\"Evaluating on hold-out test data...\")\n",
    "y_test_np, pred_test_np = get_all_preds_labels(hold_out_test_ds, model)\n",
    "\n",
    "print(\"Predictions done. Computing metrics...\")\n",
    "\n",
    "# Ensure there's data to compute metrics on\n",
    "if y_train_np.size > 0:\n",
    "    dice_train = f1_score(np.ndarray.flatten(y_train_np.astype(bool)), np.ndarray.flatten(pred_train_np > 0.5))\n",
    "    miou_train = jaccard_score(np.ndarray.flatten(y_train_np.astype(bool)), np.ndarray.flatten(pred_train_np > 0.5))\n",
    "    precision_train = precision_score(np.ndarray.flatten(y_train_np.astype(bool)), np.ndarray.flatten(pred_train_np > 0.5))\n",
    "    recall_train = recall_score(np.ndarray.flatten(y_train_np.astype(bool)), np.ndarray.flatten(pred_train_np > 0.5))\n",
    "    accuracy_train = accuracy_score(np.ndarray.flatten(y_train_np.astype(bool)), np.ndarray.flatten(pred_train_np > 0.5))\n",
    "else:\n",
    "    dice_train, miou_train, precision_train, recall_train, accuracy_train = [0]*5\n",
    "    print(\"Warning: Training data is empty, metrics set to 0.\")\n",
    "\n",
    "if y_valid_np.size > 0:\n",
    "    dice_valid = f1_score(np.ndarray.flatten(y_valid_np.astype(bool)), np.ndarray.flatten(pred_valid_np > 0.5))\n",
    "    miou_valid = jaccard_score(np.ndarray.flatten(y_valid_np.astype(bool)), np.ndarray.flatten(pred_valid_np > 0.5))\n",
    "    precision_valid = precision_score(np.ndarray.flatten(y_valid_np.astype(bool)), np.ndarray.flatten(pred_valid_np > 0.5))\n",
    "    recall_valid = recall_score(np.ndarray.flatten(y_valid_np.astype(bool)), np.ndarray.flatten(pred_valid_np > 0.5))\n",
    "    accuracy_valid = accuracy_score(np.ndarray.flatten(y_valid_np.astype(bool)), np.ndarray.flatten(pred_valid_np > 0.5))\n",
    "else:\n",
    "    dice_valid, miou_valid, precision_valid, recall_valid, accuracy_valid = [0]*5\n",
    "    print(\"Warning: Validation data is empty, metrics set to 0.\")\n",
    "\n",
    "if y_test_np.size > 0:\n",
    "    dice_test = f1_score(np.ndarray.flatten(y_test_np.astype(bool)), np.ndarray.flatten(pred_test_np > 0.5))\n",
    "    miou_test = jaccard_score(np.ndarray.flatten(y_test_np.astype(bool)), np.ndarray.flatten(pred_test_np > 0.5))\n",
    "    precision_test = precision_score(np.ndarray.flatten(y_test_np.astype(bool)), np.ndarray.flatten(pred_test_np > 0.5))\n",
    "    recall_test = recall_score(np.ndarray.flatten(y_test_np.astype(bool)), np.ndarray.flatten(pred_test_np > 0.5))\n",
    "    accuracy_test = accuracy_score(np.ndarray.flatten(y_test_np.astype(bool)), np.ndarray.flatten(pred_test_np > 0.5))\n",
    "else:\n",
    "    dice_test, miou_test, precision_test, recall_test, accuracy_test = [0]*5\n",
    "    print(\"Warning: Hold-out test data is empty, metrics set to 0.\")\n",
    "\n",
    "\n",
    "print(\"Metrics computation finished.\")\n",
    "\n",
    "final_file = f'results_{model_type}_{str(filters)}_{dataset_name_suffix}_{ct}.txt'\n",
    "print(f\"Saving results to: {final_file}\")\n",
    "\n",
    "with open(final_file, 'w') as f: # Changed to 'w' to create a new file for each run\n",
    "    f.write(f'Dataset: {dataset_name_suffix}\\n')\n",
    "    f.write(f'Model: {model_type}, Filters: {filters}\\n')\n",
    "    f.write(f'Timestamp: {ct}\\n\\n')\n",
    "    \n",
    "    f.write(f'Training Set Metrics:\\n')\n",
    "    f.write(f'  Dice (F1): {dice_train:.4f}\\n')\n",
    "    f.write(f'  mIoU: {miou_train:.4f}\\n')\n",
    "    f.write(f'  Precision: {precision_train:.4f}\\n')\n",
    "    f.write(f'  Recall: {recall_train:.4f}\\n')\n",
    "    f.write(f'  Accuracy: {accuracy_train:.4f}\\n\\n')\n",
    "\n",
    "    f.write(f'Validation Set Metrics:\\n')\n",
    "    f.write(f'  Dice (F1): {dice_valid:.4f}\\n')\n",
    "    f.write(f'  mIoU: {miou_valid:.4f}\\n')\n",
    "    f.write(f'  Precision: {precision_valid:.4f}\\n')\n",
    "    f.write(f'  Recall: {recall_valid:.4f}\\n')\n",
    "    f.write(f'  Accuracy: {accuracy_valid:.4f}\\n\\n')\n",
    "\n",
    "    f.write(f'Hold-out Test Set Metrics (from training data split):\\n')\n",
    "    f.write(f'  Dice (F1): {dice_test:.4f}\\n')\n",
    "    f.write(f'  mIoU: {miou_test:.4f}\\n')\n",
    "    f.write(f'  Precision: {precision_test:.4f}\\n')\n",
    "    f.write(f'  Recall: {recall_test:.4f}\\n')\n",
    "    f.write(f'  Accuracy: {accuracy_test:.4f}\\n\\n')\n",
    "\n",
    "print('File done.')\n",
    "\n",
    "# Optional: Predict on the final_test_ds (unlabeled data)\n",
    "# This part is for generating masks for your actual test set.\n",
    "# These masks can then be saved to disk if needed.\n",
    "if final_test_ds:\n",
    "    print(\"\\nGenerating predictions for the final (unlabeled) test set...\")\n",
    "    final_test_predictions_list = []\n",
    "    for batch_images in final_test_ds:\n",
    "        preds = model.predict_on_batch(batch_images)\n",
    "        final_test_predictions_list.append(preds)\n",
    "    \n",
    "    if final_test_predictions_list:\n",
    "        final_test_predictions_np = np.concatenate(final_test_predictions_list, axis=0)\n",
    "        print(f\"Generated {final_test_predictions_np.shape[0]} predictions for the final test set.\")\n",
    "        # Example: Save the first prediction mask\n",
    "        # if final_test_predictions_np.shape[0] > 0:\n",
    "        #     plt.imshow(final_test_predictions_np[0, :, :, 0] > 0.5, cmap='gray')\n",
    "        #     plt.title(\"Example Prediction from Final Test Set\")\n",
    "        #     plt.savefig(f'example_final_test_prediction_{ct}.png')\n",
    "        #     print(f\"Saved an example prediction mask to example_final_test_prediction_{ct}.png\")\n",
    "        # You can add code here to save all predicted masks, e.g., using tf.keras.utils.save_img or PIL.\n",
    "    else:\n",
    "        print(\"No predictions generated for the final test set (list was empty).\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo final (unlabeled) test dataset provided or it was empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
